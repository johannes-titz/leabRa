<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Johannes Titz (johannes.titz at gmail.com)" />

<meta name="date" content="2017-09-18" />

<title>LeabRa: Biologically realistic neural networks based on Leabra in R</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">LeabRa: Biologically realistic neural networks based on Leabra in R</h1>
<h4 class="author"><em>Johannes Titz (johannes.titz at gmail.com)</em></h4>
<h4 class="date"><em>2017-09-18</em></h4>



<p>This package provides the Leabra artificial neural network algorithm (O’Reilly, 1996) for R. Leabra stands for “local error driven and associative biologically realistic algorithm.” It is the Rolls Royce of artificial neural networks because it combines error driven learning and self organized learning in an elegant way, while focusing on a biologically plausible learning rule. If you have never heard of Leabra, you should read about it first. A good place to start is the computational cognitive neuroscience book (Part I), available at <a href="https://grey.colorado.edu/CompCogNeuro/index.php/CCNBook/Main" class="uri">https://grey.colorado.edu/CompCogNeuro/index.php/CCNBook/Main</a> (O’Reilly et al., 2016). This version of Leabra is rather slow compared to the original implementation in C++ (<a href="https://grey.colorado.edu/emergent/index.php/Main_Page" class="uri">https://grey.colorado.edu/emergent/index.php/Main_Page</a>). It was not intended for constructing large networks or running many trials – unless you do not care about processing resources. The main purpose of this implementation is to try out new ideas quickly, either in constructing new networks or in changing the algorithm to achieve certain outcomes. If you wanted to do this with the C++ version, you would have to deal with optimized code, which is harder to read. Note that the MATLAB version by Sergio Verduzco-Flores (<a href="https://grey.colorado.edu/svn/emergent/emergent/trunk/Matlab/" class="uri">https://grey.colorado.edu/svn/emergent/emergent/trunk/Matlab/</a>) has the same purpose, and I recommend trying it out and reading his great documentation. I still believe that this R version is the easiest way to get familiar with Leabra quickly. In contrast to MATLAB, R respects your freedom (<a href="https://www.gnu.org/philosophy/free-sw.html" class="uri">https://www.gnu.org/philosophy/free-sw.html</a>), so you can get going by simply downloading R and installing the leab<em>R</em>a package. This is especially true for psychologists, many of whom are acquainted with R already and might want to “wrangle” with the data in their programming mother tongue. (R is quite popular among psychologists.) What follows is a brief introduction for constructing networks with this package. The first network will be a pattern associator: it associates a specific input with a specific output in an error-driven fashion through a hidden layer. The second network will be a self-organized network that attempts to categorize animals represented by feature vectors.</p>
<div id="pattern-associator" class="section level2">
<h2>Pattern Associator</h2>
<p>Associating two patterns may seem unspectacular at first glance, because this can be achieved easily by back-propagating errors (Rumelhart et al., 1986). But the learning rule employed by Leabra is not only more sophisticated, it is also biologically oriented. In essence, you first present the input without the correct output. After that, you present the input and the correct output together. Now the weights are changed in such a way that the activation of the output converges to the correct output if the same input is shown again. The critical part is that changing weights is done completely locally. Only the activations of connected neurons over different time frames (short, medium, and long term) are used to achieve this.</p>
<p>I think of it as akin to learning a new language or geography. For instance, suppose I want to associate Burkina Faso with its capital Ouagadougou (pronounced waga’du:gu). In the first phase only Burkina Faso is presented and the neuronal network in my brain will try to produce the correct output, Ouagadougou. It will likely not succeed at first; maybe something similar will be output, but not the correct city. After this phase Burkina Faso and Ouagadougou are presented simultaneously. Now is the time to change the weights so that the output will be more similar to Ouagadougou. This adjustment depends only on the neuron’s activation over short, medium, and long time frames – variables that are likely available to a real biological neuron.</p>
<div id="constructing-the-network" class="section level3">
<h3>Constructing the Network</h3>
<p>Let us load the package</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(leabRa)</code></pre></div>
<p>To reproduce the example we can use a seed. Try to guess who was born on July 22nd, 1904.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">07221904</span>)</code></pre></div>
<p>To construct a network, we have to specify the dimensions of the network and the connections between the layers. We will create three layers: input, hidden, and output. They are quite small to keep calculation time low. The first layer will have dimensions of <span class="math inline">\(2 \times 5\)</span> (2 rows, 5 columns), the second of <span class="math inline">\(2 \times 10\)</span> and the third of <span class="math inline">\(2 \times 5\)</span> again. Note that these dimensions are not relevant for the algorithm, because the units are vectorized internally, so we could have specified <span class="math inline">\(1 \times 10\)</span> or <span class="math inline">\(5 \times 2\)</span> for layer 1 as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dim_lays &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">10</span>), <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">5</span>))</code></pre></div>
<p>Let us now specify the connections between these layers. Layer 1 (input) should be connected with layer 2 (hidden). Layer 3 (output) will be bidirectionally connected with layer 2. If layer j sends projections to layer i, then connections[i, j] = strength &gt; 0 and 0 otherwise. Strength specifies the relative strength of that connection with respect to the other projections to layer i. More intuitively, just look at the rows and you will see that row (layer) 2 receives from columns (layers) 1 and 3; the connection with layer 1 is 5 times stronger (<span class="math inline">\(0.2 \cdot 5 = 1\)</span>) than the connection with layer 3. Furthermore, row (layer) 3 receives from column (layer) 2. Row 1 (layer 1) does not receive from anywhere, because all connection strengths are set to 0.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">connections &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,
                        <span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.2</span>,
                        <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">nrow =</span> <span class="dv">3</span>, <span class="dt">byrow =</span> T)</code></pre></div>
<p>Note that in the current version of the package, layers are either fully connected or unconnected. If you need partially connected layers, you will need to add this functionality on your own. Now we will create a network with default parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">net &lt;-<span class="st"> </span>network<span class="op">$</span><span class="kw">new</span>(dim_lays, connections)</code></pre></div>
<p>As a side note, the package is an R6 package, a special type of object oriented programming that behaves differently from the usual R object oriented programming style (S3 or S4). You can see this because we call the method of a class with the dollar sign (network<strong>$</strong>new(…)) instead of using a generic function. Furthermore, variables are also accessed via the dollar sign instead of the at-sign @.</p>
<p><em>dim_lays</em> and <em>connections</em> is the minimum you need to specify a network. But if you are constructing more complex networks, you should pay attention to <em>g_i_gain</em>, which controls overall inhibition in a layer (inhibitory conductance gain). If this value is not set carefully, you might get unexpected results (too much or not enough activation).</p>
</div>
<div id="creating-input-patterns" class="section level3">
<h3>Creating Input Patterns</h3>
<p>Now we have a network, but no inputs. Let us create 15 random patterns with the method <em>create_inputs</em> in the network class. We want random patterns in layers 1 and 3; these are supposed to be associated during learning. We call these inputs <em>inputs_plus</em>, because these are what are presented to the network during the plus phase (correct output in layer 3 is presented). <em>prop_active</em> is the number of active units in the patterns; activation is either 0.05 or 0.95. We choose .3, meaning that on average 30% of units will have an activation of 0.95 and 70% an activation of 0.05.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inputs_plus &lt;-<span class="st"> </span>net<span class="op">$</span><span class="kw">create_inputs</span>(<span class="dt">which_layers =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>),
                                 <span class="dt">n_inputs =</span> <span class="dv">15</span>,
                                 <span class="dt">prop_active =</span> .<span class="dv">3</span>)</code></pre></div>
<p>It is possible to create inputs with your own functions. The network will accept an external input list that has a length equal to the number of layers. Every element in the list should have the activation values of the neurons for the specific layer.</p>
<p>For error-driven learning the Leabra way, we need to remove the inputs of the output layer (layer 3) for the minus phase. We will call this list <em>inputs_minus</em> (the correct output is missing, so it needs to be “subtracted”). Functionals are neat, so we will use lapply here:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inputs_minus &lt;-<span class="st"> </span><span class="kw">lapply</span>(inputs_plus, <span class="cf">function</span>(x) <span class="kw">replace</span>(x, <span class="dv">3</span>, <span class="kw">list</span>(<span class="ot">NULL</span>)))</code></pre></div>
</div>
<div id="learning" class="section level3">
<h3>Learning</h3>
<p>Now we can start learning with the default parameters. The return value of the learning function is the output activation after each trial before the weights are changed. This way we save resources, because we do not have to present the inputs again after learning. The first epoch will be an approximate baseline of each stimulus. In the next step we will use the output activations to calculate the error. During learning, the progress is reported by dots representing a single trial. This means that the minus phase, plus phase, and weight changing have been performed for one stimulus. Every row is a new epoch, which is a term to describe that all stimuli were presented once.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_epochs &lt;-<span class="st"> </span><span class="dv">10</span>
outs &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="kw">seq</span>(n_epochs), <span class="cf">function</span>(x) 
  net<span class="op">$</span><span class="kw">learn_error_driven</span>(inputs_minus,
                         inputs_plus))</code></pre></div>
<pre><code>## ...............
## ...............
## ...............
## ...............
## ...............
## ...............
## ...............
## ...............
## ...............
## ...............</code></pre>
</div>
<div id="plotting-results" class="section level3">
<h3>Plotting Results</h3>
<p>The network class can calculate the mean absolute distance (mad) with the method <em>mad_per_epoch</em> between the actual and correct patterns for each epoch. You can also use your own functions on these lists to calculate other types of errors like the cosine error. We are interested in the error of layer 3 (output).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mad &lt;-<span class="st"> </span>net<span class="op">$</span><span class="kw">mad_per_epoch</span>(outs, inputs_plus, <span class="dv">3</span>)</code></pre></div>
<p>How about a minimalist plot to see if it worked?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mad, <span class="dt">axes =</span> F, <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">family =</span> <span class="st">&quot;serif&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;epoch [#]&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;mean absolute distance [activation]&quot;</span>,
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="kw">round</span>(<span class="kw">min</span>(mad), <span class="dv">2</span>), <span class="kw">round</span>(<span class="kw">max</span>(mad <span class="op">+</span><span class="st"> </span><span class="fl">0.01</span>), <span class="dv">2</span>)))
<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at =</span> <span class="kw">seq</span>(<span class="kw">length</span>(mad)), <span class="dt">tick =</span> T, <span class="dt">family =</span> <span class="st">&quot;serif&quot;</span>)
<span class="kw">axis</span>(<span class="dv">2</span>, <span class="dt">at =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.05</span>), <span class="dt">labels =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.05</span>), <span class="dt">tick =</span> T,
     <span class="dt">family =</span> <span class="st">&quot;serif&quot;</span>, <span class="dt">las =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>The error gets smaller with each epoch, so the pattern associator seems to work just fine.</p>
</div>
<div id="some-additional-notes" class="section level3">
<h3>Some Additional Notes</h3>
<p>You can influence how many cycles should be run during the minus phase and the plus phase, which are parameters for the <em>learn_error_driven</em> method. You could also implement your own functions to learn. Internally, the <em>learn_error_driven</em> method is straightforward. It uses the method <em>cycle</em> to clamp the external input activations and to get the internal inputs from other layers. This is done several times for the minus phase (e.g. 50 times by default) and then for the plus phase (e.g. 25 times by default). After that, the method <em>chg_wt</em> is called to adjust the weights. This procedure is repeated for every stimulus.</p>
<p>If you want to modify the initial weight matrix you have several options. When creating the network, you can specify a function to create a random weight. The default function is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">w_init_fun =<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">runif</span>(x, <span class="fl">0.3</span>, <span class="fl">0.7</span>)</code></pre></div>
<p>It produces weights between 0.3 and 0.7 from a uniform distribution. Let us say you want to generate weights from a normal distribution with a mean of 0.6 and a standard deviation of 0.1. Just specify the <em>w_init_fun</em> accordingly when constructing a new network object:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">net &lt;-<span class="st"> </span>network<span class="op">$</span><span class="kw">new</span>(dim_lays, connections,
                   <span class="dt">w_init_fun =</span> <span class="cf">function</span>(x) <span class="kw">rnorm</span>(x, <span class="dt">mean =</span> <span class="fl">0.6</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>))</code></pre></div>
<p>If this does not offer enough flexibility, you can also create your own weight matrix from scratch and pass it as the parameter <em>w_init</em>, the initial weight matrix. <em>w_init</em> is a matrix of matrices (like a cell array in MATLAB):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_weights &lt;-<span class="st"> </span>net<span class="op">$</span><span class="kw">get_weights</span>()
all_weights</code></pre></div>
<pre><code>##      [,1]        [,2]        [,3]       
## [1,] NULL        NULL        NULL       
## [2,] Numeric,200 NULL        Numeric,200
## [3,] NULL        Numeric,200 NULL</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_weights[<span class="dv">3</span>, <span class="dv">2</span>]</code></pre></div>
<pre><code>## [[1]]
##            [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
##  [1,] 0.6199792 0.5816655 0.5966143 0.6089049 0.5807257 0.5352835
##  [2,] 0.5377392 0.5942340 0.5608627 0.5589160 0.4438800 0.6483404
##  [3,] 0.6602290 0.6014105 0.6947235 0.7350114 0.5999013 0.5665938
##  [4,] 0.5013222 0.5464402 0.5464375 0.6761180 0.5927200 0.6464844
##  [5,] 0.7125542 0.5597441 0.6541184 0.7898291 0.6359726 0.6202081
##  [6,] 0.6245973 0.6564946 0.6311048 0.6802373 0.6934607 0.5922013
##  [7,] 0.4933633 0.7623871 0.6622023 0.5322418 0.4851709 0.5541157
##  [8,] 0.6356127 0.6887199 0.6371301 0.7478384 0.5997549 0.6241816
##  [9,] 0.7103423 0.4964789 0.6067320 0.7706679 0.4798251 0.6227243
## [10,] 0.7072451 0.6498023 0.7569080 0.5923243 0.6646205 0.4953556
##            [,7]      [,8]      [,9]     [,10]     [,11]     [,12]
##  [1,] 0.4657841 0.4880591 0.7225643 0.4055622 0.5686968 0.5642393
##  [2,] 0.5458209 0.6754034 0.4899045 0.4762736 0.6155015 0.6982280
##  [3,] 0.5430482 0.5845956 0.4379233 0.6910951 0.5464861 0.5369073
##  [4,] 0.5609346 0.5984497 0.4077778 0.5712721 0.7759060 0.6386810
##  [5,] 0.4486378 0.5860919 0.6365965 0.5661420 0.4844575 0.5107766
##  [6,] 0.4439360 0.6209044 0.6283859 0.4777480 0.7109740 0.4373598
##  [7,] 0.6267409 0.5056140 0.5632133 0.7094829 0.5958420 0.4406635
##  [8,] 0.6970609 0.6294668 0.4147536 0.6806164 0.7446742 0.7294338
##  [9,] 0.5323914 0.8517820 0.5133184 0.4869751 0.5099135 0.6131721
## [10,] 0.7229776 0.5214591 0.6332850 0.5893964 0.4716185 0.5320660
##           [,13]     [,14]     [,15]     [,16]     [,17]     [,18]
##  [1,] 0.4239211 0.6668740 0.6651165 0.5915938 0.5722867 0.4240229
##  [2,] 0.6572897 0.6708034 0.6310316 0.3660084 0.7316463 0.7295660
##  [3,] 0.5677364 0.7676059 0.5588645 0.6585694 0.5487881 0.5413860
##  [4,] 0.4914294 0.4506926 0.5760387 0.5909589 0.6628934 0.5140708
##  [5,] 0.5495123 0.5132568 0.6472836 0.6800647 0.4999588 0.7499881
##  [6,] 0.5861824 0.5952372 0.5133139 0.6587509 0.7144574 0.6387816
##  [7,] 0.5304132 0.5694711 0.6346313 0.5565570 0.5557743 0.6464434
##  [8,] 0.5989620 0.6050367 0.5551266 0.6165508 0.4864436 0.5043580
##  [9,] 0.5850210 0.6888675 0.7597975 0.4697959 0.6854917 0.5578774
## [10,] 0.7583314 0.6909298 0.6063724 0.5446131 0.6422718 0.7091008
##           [,19]     [,20]
##  [1,] 0.7470220 0.5926404
##  [2,] 0.6023885 0.5333771
##  [3,] 0.6978501 0.3738139
##  [4,] 0.6481803 0.9615503
##  [5,] 0.3121930 0.5945361
##  [6,] 0.5437668 0.4853428
##  [7,] 0.6345784 0.6550956
##  [8,] 0.5550953 0.6028935
##  [9,] 0.4613824 0.5491546
## [10,] 0.6528826 0.4835204</code></pre>
<p>Be careful when you create a <em>w_init</em> matrix on your own.</p>
<p>As mentioned before, this package uses R6 classes, meaning that you do not have to assign objects in the usual R way. For instance, calling net$learn_error_driven above actually modified the net object, although we did not make any explicit assignment. This is unusual for R and has some disadvantages, but it is faster and uses fewer resources (ideal for a simulation) than the more common S3/S4 classes. Just pay attention when you call methods in this package. They will modify objects in place.</p>
</div>
</div>
<div id="hello-world-in-connectionism-categorizing-animals" class="section level2">
<h2>Hello World in “Connectionism”: Categorizing Animals</h2>
<p>Every time I explore a new neural network software, I try to create some typical examples. One obvious example is the pattern associator. Personally, I like the example by Knight (1990, p. 70) for unsupervised (self-organized) learning of animals. This became my “hello world” for artificial neural networks.</p>
<p>Again, let us set a seed, so you can reproduce the example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">22071904</span>)</code></pre></div>
<p>We will start with the input patterns, because the network architecture depends on the dimension of these patterns.</p>
<div id="input-patterns" class="section level3">
<h3>Input Patterns</h3>
<p>The inputs for the network are animals represented by features that are either present or absent (Knight, 1990, p. 71). This data comes directly with the leab<em>R</em>a package and is called <em>animals</em>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">animals</code></pre></div>
<pre><code>##           has.hair has.scales has.feathers flies lives.in.water lays.eggs
## dog              1          0            0     0              0         0
## cat              1          0            0     0              0         0
## bat              1          0            0     1              0         0
## whale            1          0            0     0              1         0
## canary           0          0            1     1              0         1
## robin            0          0            1     1              0         1
## ostrich          0          0            1     0              0         1
## snake            0          1            0     0              0         1
## lizard           0          1            0     0              0         1
## alligator        0          1            0     0              1         1</code></pre>
<p>Because the network class at present only accepts a list as external inputs, we transform the data frame rows into elements of a list.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inputs &lt;-<span class="st"> </span>plyr<span class="op">::</span><span class="kw">alply</span>(animals, <span class="dv">1</span>)</code></pre></div>
<p>Furthermore, we need an empty list element (NULL) for the second layer.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inputs &lt;-<span class="st"> </span><span class="kw">lapply</span>(inputs, <span class="cf">function</span>(x) <span class="kw">list</span>(x, <span class="ot">NULL</span>))</code></pre></div>
<p>This is what I meant when I wrote that R people might prefer wrangling with data in their mother tongue.</p>
</div>
<div id="network-architecture" class="section level3">
<h3>Network Architecture</h3>
<p>We will use a 2-layer network, where layer 2 receives projections from layer 1. The size of layer 1 must be 6, because there are 6 features for representing an animal in our example. The size of layer 2 is 3, meaning that the inputs will be categorized into three groups (the active unit will be the category). You can experiment with the number of units in layer 2 to get other categories.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dim_lays &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="dv">6</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>))
connections &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>,
                        <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> T)</code></pre></div>
</div>
<div id="learning-1" class="section level3">
<h3>Learning</h3>
<p>We want to run the simulation not just once, but several times to get a feeling for how much the results can vary. To achieve this, we can write a short function that initializes the network and then learns unsupervised. After the learning is done, we test the network’s reactions to the shown inputs with the method <em>test_inputs</em> (changing weights is turned off in this method). In contrast to the network described previously, we have to do this because the learning phase will only last one epoch per simulation. The network will be different for each simulation, because the weights are initialized randomly. You can think of this procedure as having several participants observe ten different animals. The differences between participants are indicated by the individual weight matrices assigned to each network.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">run_sim &lt;-<span class="st"> </span><span class="cf">function</span>(dim_lays, connections, inputs){
  net &lt;-<span class="st"> </span>network<span class="op">$</span><span class="kw">new</span>(dim_lays, connections)
  net<span class="op">$</span><span class="kw">learn_self_organized</span>(inputs, <span class="dt">random_order =</span> <span class="ot">TRUE</span>)
  <span class="kw">return</span>(net<span class="op">$</span><span class="kw">test_inputs</span>(inputs))
}</code></pre></div>
<p>Now we can run the simulation. Ten runs should not be a problem, because the network is tiny.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_runs &lt;-<span class="st"> </span><span class="dv">10</span>
outs &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="kw">seq</span>(n_runs), <span class="cf">function</span>(x) <span class="kw">run_sim</span>(dim_lays, connections, inputs))</code></pre></div>
<pre><code>## ..........
## ..........
## ..........
## ..........
## ..........
## ..........
## ..........
## ..........
## ..........
## ..........</code></pre>
</div>
<div id="plotting-results-1" class="section level3">
<h3>Plotting Results</h3>
<p>The output for each run is the activations of each layer after all stimuli have been presented once. We are only interested in layer 2, so let us extract these activations and transform them into data frames (some “wrangling” again). We can then look at the outputs of two simulation runs to get a feeling for whether it worked.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">outs_layer_two &lt;-<span class="st"> </span><span class="kw">lapply</span>(outs, <span class="cf">function</span>(x) <span class="kw">lapply</span>(x, <span class="cf">function</span>(y) y[[<span class="dv">2</span>]]))
outs_layer_two &lt;-<span class="st"> </span><span class="kw">lapply</span>(outs_layer_two, <span class="cf">function</span>(x) <span class="kw">do.call</span>(rbind, x))
outs_layer_two &lt;-<span class="st"> </span><span class="kw">lapply</span>(outs_layer_two, round, <span class="dv">2</span>)</code></pre></div>
<p>To inspect the third simulation we just call:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">outs_layer_two[[<span class="dv">3</span>]]</code></pre></div>
<pre><code>##    [,1] [,2] [,3]
## 1  0.00 0.00 0.92
## 2  0.00 0.00 0.92
## 3  0.00 0.00 0.44
## 4  0.08 0.00 0.75
## 5  0.00 0.94 0.00
## 6  0.00 0.94 0.00
## 7  0.00 0.88 0.00
## 8  0.91 0.18 0.00
## 9  0.91 0.18 0.00
## 10 0.97 0.00 0.00</code></pre>
<p>The output units fight for activation, such that only one unit is active most of the time. This is the category of the animal and it seems to work quite well. For instance, recall that the animals in rows 5, 6, and 7 were canary, robin, and ostrich and they all have high activations on unit 2. Let us look at another simulation, where the result is not as straightforward:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">outs_layer_two[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>##    [,1] [,2] [,3]
## 1  0.00 0.89    0
## 2  0.00 0.89    0
## 3  0.00 0.72    0
## 4  0.00 0.96    0
## 5  0.88 0.00    0
## 6  0.88 0.00    0
## 7  0.74 0.06    0
## 8  0.71 0.00    0
## 9  0.71 0.00    0
## 10 0.00 0.75    0</code></pre>
<p>One problem we can see here is that only 2 output units are active. This happens because of “hogging,” a problem that often occurs in self organized learning (e.g. Knight, 1990, p. 72). Some output units are so strong that they attract everything. This can also happen with a single unit. There are a couple of ways to deal with hogging (see <a href="https://grey.colorado.edu/emergent/index.php/Leabra" class="uri">https://grey.colorado.edu/emergent/index.php/Leabra</a>), but for our simple example we can simply ignore it; we have run several simulations, so it is not an issue if a couple of them have hogging units. Maybe this also reflects that grouping animals is to some degree subjective and that sometimes only two categories emerge.</p>
<p>There are many ways to work with these output activations. For instance, we can calculate the distance between the ten animals in their output activation and then run a cluster analysis or draw a heatmap. But a devil’s advocate might say that this allows for too many degrees of freedom. The output units can have activations between 0 and 1 and there are three of them. Maybe the 6 binary features will just be mapped onto three units which have a wide range of possible values. This might not be terribly impressive. Instead we can try to force the network to make a clear decision in which category to put an animal (one, two, or three). This is also more similar to how we would prompt human participants in a cognitive experiment. For instance, we could ask them to group animals into three categories, with every animal in exactly one category.</p>
<p>To achieve this, we can transform the activation matrices to 1 and 0. The maximum value will get a value of 1 and the rest of 0. This is a clear-cut decision into which category to put an animal. We will use a short function for this, that is applied on every row of every output matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">apply_threshold_on_row &lt;-<span class="st"> </span><span class="cf">function</span>(row){
  row[<span class="op">-</span><span class="kw">which.max</span>(row)] &lt;-<span class="st"> </span><span class="dv">0</span>
  row[<span class="kw">which.max</span>(row)] &lt;-<span class="st"> </span><span class="dv">1</span>
  <span class="kw">return</span>(row)
}

outs_layer_two &lt;-<span class="st"> </span><span class="kw">lapply</span>(outs_layer_two,
                         <span class="cf">function</span>(x) <span class="kw">t</span>(<span class="kw">apply</span>(x, <span class="dv">1</span>, apply_threshold_on_row)))
outs_layer_two[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>##    [,1] [,2] [,3]
## 1     0    1    0
## 2     0    1    0
## 3     0    1    0
## 4     0    1    0
## 5     1    0    0
## 6     1    0    0
## 7     1    0    0
## 8     1    0    0
## 9     1    0    0
## 10    0    1    0</code></pre>
<p>Now we want to know which animals are grouped together. Here, we take a shortcut by calculating the binary distance matrix for every simulation. Using the value assignments described in the previous paragraph, we know the distance between two animals is either 0 if they belong to the same category or 1 if they do not.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dists &lt;-<span class="st"> </span><span class="kw">lapply</span>(outs_layer_two, dist, <span class="dt">method =</span> <span class="st">&quot;binary&quot;</span>)
dists[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>##    1 2 3 4 5 6 7 8 9
## 2  0                
## 3  0 0              
## 4  0 0 0            
## 5  1 1 1 1          
## 6  1 1 1 1 0        
## 7  1 1 1 1 0 0      
## 8  1 1 1 1 0 0 0    
## 9  1 1 1 1 0 0 0 0  
## 10 0 0 0 0 1 1 1 1 1</code></pre>
<p>So here animals 2, 3, 4, and 10 are in one category and the rest are in the other. But this is only 1 distance matrix; we have 10 of them, which is simply too much information. We can average these values over the simulation runs by using a neat functional again:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dists_mtrx &lt;-<span class="st"> </span><span class="kw">lapply</span>(dists, as.matrix)
mean_dists &lt;-<span class="st"> </span><span class="kw">Reduce</span>(<span class="st">&quot;+&quot;</span>, dists_mtrx) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(dists)
mean_dists</code></pre></div>
<pre><code>##      1   2   3   4   5   6   7   8   9  10
## 1  0.0 0.0 0.2 0.2 0.6 0.6 0.7 0.9 0.9 0.8
## 2  0.0 0.0 0.2 0.2 0.6 0.6 0.7 0.9 0.9 0.8
## 3  0.2 0.2 0.0 0.4 0.4 0.4 0.5 0.8 0.8 0.7
## 4  0.2 0.2 0.4 0.0 0.8 0.8 0.8 0.9 0.9 0.8
## 5  0.6 0.6 0.4 0.8 0.0 0.0 0.1 0.7 0.7 0.8
## 6  0.6 0.6 0.4 0.8 0.0 0.0 0.1 0.7 0.7 0.8
## 7  0.7 0.7 0.5 0.8 0.1 0.1 0.0 0.6 0.6 0.7
## 8  0.9 0.9 0.8 0.9 0.7 0.7 0.6 0.0 0.0 0.2
## 9  0.9 0.9 0.8 0.9 0.7 0.7 0.6 0.0 0.0 0.2
## 10 0.8 0.8 0.7 0.8 0.8 0.8 0.7 0.2 0.2 0.0</code></pre>
<p>We need to add the row names from the original data set, so that we know which animal is which.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colnames</span>(mean_dists) &lt;-<span class="st"> </span><span class="kw">rownames</span>(animals)
<span class="kw">rownames</span>(mean_dists) &lt;-<span class="st"> </span><span class="kw">rownames</span>(animals)</code></pre></div>
<p>We are finally ready to apply clustering and then plot a dendrogram:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">hclust</span>(<span class="kw">as.dist</span>(mean_dists)), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Distance&quot;</span>)</code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>Three natural categories seem to emerge. The distance between two animals in each category is always zero, which means they are identical (1. snake and lizard; 2. canary and robin; 3. dog and cat). The more interesting part is what happens with the alligator, ostrich, whale, and bat. The alligator is grouped with snake and lizard. These are the reptiles, although the alligator is not a typical member because it lives in water. The ostrich is grouped with canary and robin, the birds. Although it cannot fly, it still makes sense to put the ostrich in this category. Finally, the whale and the bat are grouped together with dog and cat, the mammals. They are rather untypical members of this category, but zoologists also group them this way. Obviously, the example by Knight is somewhat artificial, but in this sense it is my favorite “hello world” example for artificial neural networks.</p>
</div>
</div>
<div id="summary-and-restrictions" class="section level2">
<h2>Summary and Restrictions</h2>
<p>These examples show that leab<em>R</em>a seems to work fine for two typical use cases. Still, I cannot guarantee that the code is correct in every detail. Furthermore, there are some differences from the original C++ code (as of the time of writing this vignette, September 2017). For instance, you cannot specify partial connections and the nxx1-function is a step-function to reduce calculation resources. What is more, compared to the default in the current emergent version, the R version does not use momentum for calculating weight changes. Overall, the algorithm should still produce very similar results to the original Leabra implementation.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Knight, K. (1990). Connectionist ideas and algorithms. <em>Communications of the ACM</em>, <em>33</em>(11), 59–74.</p>
<p>O’Reilly, R. C. (1996). <em>The Leabra Model of Neural Interactions and Learning in the Neocortex</em>. Phd Thesis, Carnegie Mellon University, Pittsburgh. URL: <a href="ftp://grey.colorado.edu/pub/oreilly/thesis/oreilly_thesis.all.pdf" class="uri">ftp://grey.colorado.edu/pub/oreilly/thesis/oreilly_thesis.all.pdf</a></p>
<p>O’Reilly, R. C., Munakata, Y., Frank, M. J., Hazy, T. E., and Contributors (2016). <em>Computational Cognitive Neuroscience</em>. Wiki Book, 3rd (partial) Edition. URL: <a href="http://ccnbook.colorado.edu" class="uri">http://ccnbook.colorado.edu</a></p>
<p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. <em>Nature</em>. <em>323</em>(6088): 533–536. <a href="doi:10.1038/323533a0" class="uri">doi:10.1038/323533a0</a>.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
